---
title: Scene Understanding using VideoCLIP
---

# Understanding Video Scenes using VideoCLIP

 OpenAI introduces their new model which is called CLIP (Contrastive Language-Image Pre-training) in the paper Learning Transferable Visual Models From Natural Language Supervision paper. 
 
 This model learns the relationship between a whole sentence and the image it describes; Given an input sentence it will be able to retrieve the most related images corresponding to that sentence. The system is trained on full sentences instead of single classes like car, dog, etc. The intuition is that when trained on whole sentences, the model can learn a lot more things and finds some pattern between images and texts.

 ## VideoCLIP

[VideoCLIP: Contrastive Pre-training for Zero-shot Video-Text Understanding](https://arxiv.org/abs/2109.14084)



