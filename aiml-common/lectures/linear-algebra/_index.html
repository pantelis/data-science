
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Linear Algebra for Machine Learning &#8212; Introduction to Data Science</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="../../../_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="../../../_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
    <link rel="stylesheet" href="../../../_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/tabs.css" />
    <link rel="stylesheet" type="text/css" href="../../../_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
    <script src="../../../_static/jquery.js"></script>
    <script src="../../../_static/underscore.js"></script>
    <script src="../../../_static/doctools.js"></script>
    <script src="../../../_static/clipboard.min.js"></script>
    <script src="../../../_static/copybutton.js"></script>
    <script src="../../../_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script src="../../../_static/tabs.js"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../../../_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="../../../_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"tex": {"macros": {"floor": ["\\lfloor#1\\rfloor", 1], "bmat": ["\\left[\\begin{array}"], "emat": ["\\end{array}\\right]"]}}, "options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="canonical" href="https://pantelis.github.io/data-science/aiml-common/lectures/linear-algebra/_index.html" />
    <link rel="index" title="Index" href="../../../genindex.html" />
    <link rel="search" title="Search" href="../../../search.html" />
    <link rel="next" title="Calculus" href="../ml-math/calculus/_index.html" />
    <link rel="prev" title="Math for ML" href="../ml-math/_index.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint" id="site-navigation">
    <div class="bd-sidebar__content">
        <div class="bd-sidebar__top"><div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search the docs ..." aria-label="Search the docs ..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Syllabus
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../syllabus/_index.html">
   Syllabus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Data Science
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ai-intro/course-introduction/_index.html">
   Course Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ai-intro/data-science-360/_index.html">
   Data Science 360
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ai-intro/pipelines/_index.html">
   ML Pipelines
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../uber-ml-arch-case-study/_index.html">
   A Case Study of an ML Architecture - Uber
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probabilistic Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../ml-math/probability/_index.html">
   Probability Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../entropy/_index.html">
   Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pgm/bayesian-inference/_index.html">
   Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../classification/covid19-antibody-test/_index.html">
   COVID-19 Antibody Test
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pgm/bayesian-coin/_index.html">
   Bayesian Coin Flipping
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The Learning Problem
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../learning-problem/_index.html">
   The Learning Problem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../regression/linear-regression/linear_regression.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../optimization/sgd/_index.html">
   Stochastic Gradient Descent
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Classification
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../classification/classification-intro/_index.html">
   Introduction to Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../classification/logistic-regression/_index.html">
   Logistic Regression
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Trees and Intelligence of the Crowds
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../decision-trees/_index.html">
   Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../regression-trees/regression_trees.html">
   Regression tree stumps
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ensemble/_index.html">
   Ensemble Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ensemble/random-forests/_index.html">
   Random Forests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ensemble/adaboost/index.html">
   Adaptive Boosting (AdaBoost)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ensemble/gradient-boosting/index.html">
   Gradient Boosting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../ensemble/boosting-workshop/_index.html">
   Boosting workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Non-Parametric Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../unsupervised/k-means/_index.html">
   K-means Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../density-estimation/knn/_index.html">
   k-Nearest Neighbors (kNN) Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../density-estimation/knn-workshop/_index.html">
   kNN Workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../classification/perceptron/_index.html">
   The Neuron (Perceptron)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dnn/dnn-intro/_index.html">
   Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dnn/backprop-intro/_index.html">
   Introduction to Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dnn/backprop-dnn/_index.html">
   Backpropagation in Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dnn/backprop-dnn-exercises/_index.html">
   Backpropagation DNN exercises
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../dnn/fashion-mnist-case-study.html">
   Fashion MNIST Case Study
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../optimization/regularization/_index.html">
   Regularization in Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cnn/cnn-intro/_index.html">
   Introduction to Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cnn/cnn-layers/_index.html">
   CNN Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cnn/cnn-example-architectures/_index.html">
   CNN Example Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../cnn/cnn-example-architectures/using_convnets_with_small_datasets.html">
   Using convnets with small datasets
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dimensionality Reduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../recommenders/recommenders-intro/_index.html">
   Introduction to Recommender Systems
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../recommenders/netflix/_index.html">
   The Netflix Prize and Singular Value Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../pca/_index.html">
   Principal Component Analysis (PCA)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math Background
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1 current active has-children">
  <a class="reference internal" href="../ml-math/_index.html">
   Math for ML
  </a>
  <input checked="" class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/>
  <label for="toctree-checkbox-1">
   <i class="fas fa-chevron-down">
   </i>
  </label>
  <ul class="current">
   <li class="toctree-l2 current active">
    <a class="current reference internal" href="#">
     Linear Algebra for Machine Learning
    </a>
   </li>
   <li class="toctree-l2">
    <a class="reference internal" href="../ml-math/calculus/_index.html">
     Calculus
    </a>
   </li>
  </ul>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../resources/environment/index.html">
   Your Programming Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../python/_index.html">
   Learn Python
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Assignments &amp; Projects
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../assignments/probability/probability-assignment-5/index.html">
   Probability Assignment
  </a>
 </li>
</ul>

    </div>
</nav></div>
        <div class="bd-sidebar__bottom">
             <!-- To handle the deprecated key -->
            
            <div class="navbar_extra_footer">
            Theme by the <a href="https://ebp.jupyterbook.org">Executable Book Project</a>
            </div>
            
        </div>
    </div>
    <div id="rtd-footer-container"></div>
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
        <label for="__navigation"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="right"
title="Toggle navigation"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-bars"></i>
  </span>

</label>

        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/pantelis/data-science"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/pantelis/data-science/issues/new?title=Issue%20on%20page%20%2Faiml-common/lectures/linear-algebra/_index.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>

<div class="menu-dropdown menu-dropdown-download-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Download this page">
      <i class="fas fa-download"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="../../../_sources/aiml-common/lectures/linear-algebra/_index.md"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Download source file"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="headerbtn__text-container">.md</span>
</a>

      </li>
      
      <li>
        
<button onclick="printPdf(this)"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="left"
title="Print to PDF"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="headerbtn__text-container">.pdf</span>
</button>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-points">
   Key Points
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#projections">
     Projections
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-four-fundamental-subspaces">
     The Four Fundamental Subspaces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalues-and-eigenvectors">
     Eigenvalues and Eigenvectors
    </a>
   </li>
  </ul>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>Linear Algebra for Machine Learning</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#key-points">
   Key Points
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#projections">
     Projections
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-four-fundamental-subspaces">
     The Four Fundamental Subspaces
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#eigenvalues-and-eigenvectors">
     Eigenvalues and Eigenvectors
    </a>
   </li>
  </ul>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="linear-algebra-for-machine-learning">
<h1>Linear Algebra for Machine Learning<a class="headerlink" href="#linear-algebra-for-machine-learning" title="Permalink to this headline">#</a></h1>
<p>Let me introduce you MIT prof G Strang - probably the best educator in America. He has published this playlist of <a class="reference external" href="https://www.youtube.com/watch?v=Cx5Z-OslNWE&amp;list=PLUl4u3cNGP63oMNUHXqIUcrkS2PivhN3k">youtube videos on Linear Algebra</a>.</p>
<p>Also, watch <a class="reference external" href="https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab">these videos</a> for a more elementary treatment of the topic.</p>
<div class="video_wrapper" style="">
<iframe allowfullscreen="true" src="https://www.youtube.com/embed/wu3E38nguG4" style="border: 0; height: 345px; width: 560px">
</iframe></div><p><em>Linear Algebra recitation for my classes. Recitation was delivered by my TA Shweta Selvaraj Achary.</em></p>
<p>The corresponding chapter of Ian Goodfellow’s Deep Learning book is what you partially need to know as data scientists at a graduate level but arguably if you are just starting you ought to know 2.1-2.5.</p>
<iframe src="https://www.deeplearningbook.org/contents/linear_algebra.html" width="800" height="1200"></iframe>
<section id="key-points">
<h2>Key Points<a class="headerlink" href="#key-points" title="Permalink to this headline">#</a></h2>
<p>We can now summarize the points to pay attention to, for ML applications.  In the following we assume a data matrix <span class="math notranslate nohighlight">\(A\)</span> with <span class="math notranslate nohighlight">\(m\)</span> rows and <span class="math notranslate nohighlight">\(n\)</span> columns. We also assume that the matrix is such that it has <span class="math notranslate nohighlight">\(r\)</span> independent rows or columns, called <em>the matrix rank</em>.</p>
<section id="projections">
<h3>Projections<a class="headerlink" href="#projections" title="Permalink to this headline">#</a></h3>
<p>Its important to understand this basic operator and its geometric interpretation as it is met in problems like Ordinary Least Squares but also all over ML and other fields such as compressed sensing. In the following we assume that the reader is familiar with the concept of vector spaces and subspaces.</p>
<p>Let <span class="math notranslate nohighlight">\(S\)</span> be a vector subspace of <span class="math notranslate nohighlight">\(R^n\)</span>. For example in <span class="math notranslate nohighlight">\(R^3\)</span>, <span class="math notranslate nohighlight">\(S\)</span> are the lines and planes going through the origin. The projection operator onto <span class="math notranslate nohighlight">\(S\)</span> implements a linear transformation: <span class="math notranslate nohighlight">\(\Pi_S: R^3 →S\)</span>. We will stick to <span class="math notranslate nohighlight">\(R^3\)</span> to maintain the ability to plot the operations involved. We also define the orthogonal subspace,</p>
<div class="math notranslate nohighlight">
\[S^\perp  ≡  \{ \mathbf w \in R^3 | \mathbf w ^T \mathbf s = 0, ∀ \mathbf s \in S \} \]</div>
<p>The transformation <span class="math notranslate nohighlight">\(\Pi_S\)</span> projects onto space <span class="math notranslate nohighlight">\(S\)</span> in the sense that when you apply this operator, every vector <span class="math notranslate nohighlight">\(\mathbf u\)</span> in any other space results in the subspace <span class="math notranslate nohighlight">\(S\)</span>. In our example above,</p>
<div class="math notranslate nohighlight">
\[\Pi_S(\mathbf u) \in S, \forall \mathbf u \in R^3\]</div>
<p>This means that any components of the vector <span class="math notranslate nohighlight">\(\mathbf u\)</span> that belonged to <span class="math notranslate nohighlight">\(S^\perp\)</span> are gone when applying the projection operator. Effectively, the original space is decomposed into</p>
<div class="math notranslate nohighlight">
\[ R^3 = S \oplus S^\perp \]</div>
<p>Now we can treat projections onto specific subspaces such as lines and planes passing through the origin.</p>
<p>For a line defined by a direction vector <span class="math notranslate nohighlight">\(\mathbf u\)</span></p>
<div class="math notranslate nohighlight">
\[l = \{  (x,y,z) \in \R^3 | (x,y,z) = \mathbf 0 + t \mathbf u \} \]</div>
<p>we can define the projection onto the line</p>
<p><img alt="line-projection" src="../../../_images/line-projection.png" />
<em>Projection of <span class="math notranslate nohighlight">\(\mathbf u\)</span> onto the line <span class="math notranslate nohighlight">\(l\)</span></em></p>
<p>The space <span class="math notranslate nohighlight">\(S^\perp ≡ l^\perp\)</span> is a plane since it consists of all the vectors that are perpendicular to the line. What is shown in the figure as a dashed line is simply the projection of <span class="math notranslate nohighlight">\(\mathbf u\)</span> on the <span class="math notranslate nohighlight">\(l^\perp\)</span> subspace,</p>
<div class="math notranslate nohighlight">
\[\begin{split}l^\perp = \{  (x,y,z) \in \R^3 | \begin{bmatrix} x \\ y \\ z \end{bmatrix}^T  \mathbf v = 0\} \end{split}\]</div>
<p>The orthogonal space of a line with direction vector <span class="math notranslate nohighlight">\(\mathbf v\)</span> is a <em>plane</em> with a normal vector <span class="math notranslate nohighlight">\(\mathbf v\)</span>. So when we project the <span class="math notranslate nohighlight">\(\mathbf v\)</span> on the line we get two components one is lying on the line and is the <span class="math notranslate nohighlight">\(\Pi_l \mathbf u\)</span> and the other is the vector <span class="math notranslate nohighlight">\(\mathbf w\)</span> = <span class="math notranslate nohighlight">\(\Pi_{l^\perp} \mathbf u = \mathbf u - \mathbf v = \mathbf u - \Pi_{\mathbf v} \mathbf u \)</span>. The vector <span class="math notranslate nohighlight">\(\mathbf w\)</span> is what remains when we remove the projected on <span class="math notranslate nohighlight">\(\mathbf v\)</span> part from the <span class="math notranslate nohighlight">\(\mathbf u\)</span>.</p>
</section>
<section id="the-four-fundamental-subspaces">
<h3>The Four Fundamental Subspaces<a class="headerlink" href="#the-four-fundamental-subspaces" title="Permalink to this headline">#</a></h3>
<p><img alt="Four fundamental spaces" src="../../../_images/four-fundamental-spaces-linear-alg.png" /></p>
<p>The <em>fundamental theorem of Linear Algebra</em> specifies the effect of the multiplication operation of the matrix and a vector (<span class="math notranslate nohighlight">\(A\mathbf{x}\)</span>). The matrix gives raise to 4 subspaces:</p>
<ol class="simple">
<li><p><strong>The column space of <span class="math notranslate nohighlight">\(A\)</span></strong>, denoted by <span class="math notranslate nohighlight">\(\mathcal{R}(A)\)</span>, with dimension <span class="math notranslate nohighlight">\(r\)</span>.</p></li>
<li><p><strong>The nullspace of <span class="math notranslate nohighlight">\(A\)</span></strong>, denoted by <span class="math notranslate nohighlight">\(\mathcal{N}(A)\)</span>, with dimension <span class="math notranslate nohighlight">\(n-r\)</span>.</p></li>
<li><p><strong>The row space of <span class="math notranslate nohighlight">\(A\)</span></strong> which is the column space of <span class="math notranslate nohighlight">\(A^T\)</span>, with dimension <span class="math notranslate nohighlight">\(r\)</span></p></li>
<li><p><strong>The left nullspace of <span class="math notranslate nohighlight">\(A\)</span></strong>, which is the nullspace of <span class="math notranslate nohighlight">\(A^T\)</span>, denoted by <span class="math notranslate nohighlight">\(\mathcal{N}(A^T)\)</span>, with dimension <span class="math notranslate nohighlight">\(m-r\)</span>.</p></li>
</ol>
<p>The real action that the matrix performs is to <strong>transform</strong> its row space to its column space.</p>
<p>The type of matrices that are common in ML are those that the number of rows <span class="math notranslate nohighlight">\(m\)</span> representing observations is much larger than the number of columns <span class="math notranslate nohighlight">\(n\)</span> that represent features. We will call these matrices “tall” for obvious reasons. Let us consider one trivial but instructive example of the smallest possible “tall” matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix} a_{11} &amp; a_{12} \\ a_{21}  &amp; a_{22} \\ a_{31} &amp; a_{32} \end{bmatrix} = \begin{bmatrix} 1       &amp; 0 \\ 5       &amp; 4 \\ 2       &amp; 4 \end{bmatrix}\end{split}\]</div>
<p>In ML we are usually concerned with the problem of learning the weights <span class="math notranslate nohighlight">\(x_1, x_2\)</span> that will combine the features and result into the given target variables <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. The notation here is different and we have adopted the notation of many linear algebra textbooks.</p>
<div class="math notranslate nohighlight">
\[\begin{split} \begin{bmatrix} 1       &amp; 0 \\ 5       &amp; 4 \\ 2       &amp; 4 \end{bmatrix}  \begin{bmatrix} x_1 \\ x_2  \end{bmatrix} =
\begin{bmatrix} b_1 \\ b_2 \\  b_3  \end{bmatrix}\end{split}\]</div>
<p>To make more explicit the combination of features we can write,</p>
<div class="math notranslate nohighlight">
\[\begin{split} x_1 \begin{bmatrix} 1 \\ 5 \\ 2 \end{bmatrix} + x_2 \begin{bmatrix} 0 \\ 4 \\  4  \end{bmatrix} = \begin{bmatrix} b_1 \\ b_2 \\  b_3  \end{bmatrix}\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(m=3 &gt; n=2\)</span>, we have more equations than unknowns we in general we have no solutions - a system with <span class="math notranslate nohighlight">\(m &gt; n\)</span> will be solvable only for certain right hand sides <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. Those are all the vectors <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> that lie in the column space of <span class="math notranslate nohighlight">\(A\)</span>.</p>
<p><img alt="column-space" src="../../../_images/column-space.png" /></p>
<p>In this example, as shown in the picture <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> must lie in the plane spanned by the two columns of <span class="math notranslate nohighlight">\(A\)</span>. The plane is a subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^m=\mathbb{R}^3\)</span> in this case.</p>
<p>Now instead of looking at what properties <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> must have for the system to have a solution, lets look at the <em>dual</em> problem i.e. what weights <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> can attain those <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>. The right-hand side <span class="math notranslate nohighlight">\(\mathbf{b}=0\)</span> always allows the solution <span class="math notranslate nohighlight">\(\mathbf{x}=0\)</span>
The solutions to <span class="math notranslate nohighlight">\(A \mathbf{x} = \mathbf{0}\)</span> form a vector space - <strong>the nullspace</strong> <span class="math notranslate nohighlight">\(\mathcal{N}(A)\)</span>. The nullspace is also called the <em>kernel</em> of matrix <span class="math notranslate nohighlight">\(A\)</span> and the its dimension <span class="math notranslate nohighlight">\(n-r\)</span> is called the nullity.</p>
<p><span class="math notranslate nohighlight">\(\mathcal{N}(A)\)</span> is a subspace of <span class="math notranslate nohighlight">\(\mathbb{R}^n=\mathbb{R}^2\)</span> in this case. For our specific example,</p>
<div class="math notranslate nohighlight">
\[\begin{split} x_1 \begin{bmatrix} 1 \\ 5 \\ 2 \end{bmatrix} + x_2 \begin{bmatrix} 0 \\ 4 \\  4  \end{bmatrix} = \begin{bmatrix} 0 \\ 0 \\  0  \end{bmatrix}\end{split}\]</div>
<p>the only solution that can satisfy this set of homogenous equations is: <span class="math notranslate nohighlight">\(\mathbf{x}=\mathbf{0}\)</span> and this means that the null space contains only the zero vector and this</p>
<p>Two vectors are independent when their linear combination cannot be zero, unless both <span class="math notranslate nohighlight">\(x_1\)</span> and <span class="math notranslate nohighlight">\(x_2\)</span> are zero.  The columns of <span class="math notranslate nohighlight">\(A\)</span> are therefore linearly independent and they span the column space. They have therefore all the properties needed for them to constitute a set called the <em>basis</em> for that space and we have two basis vectors (the rank is <span class="math notranslate nohighlight">\(r=2\)</span> in this case). The dimension of the column space is in fact the same as the dimension of the row space (<span class="math notranslate nohighlight">\(r\)</span>) and the mapping from row space to column space is in fact invertible. Every vector <span class="math notranslate nohighlight">\(\mathbf{b}\)</span> comes from one and only one vector <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> of the row space (<span class="math notranslate nohighlight">\(\mathbf{x}_r\)</span>). And this vector can be found by the inverse operation - noting that only the inverse <span class="math notranslate nohighlight">\(A^{-1}\)</span> is the operation that moves the vector correctly from the column space to the row space. The inverse exists only if <span class="math notranslate nohighlight">\(r=m=n\)</span> - this is important as in most ML problems we are dealing with “tall” matrices with the number of equations much larger than the number of unknowns which makes the system <em>inconsistent</em> (or <em>degenerate</em>).</p>
<p><img alt="projection-column-space" src="../../../_images/projection-column-space.png" />
<em>Projection onto the column space</em></p>
<p>Geometrically you can think about the basis vectors as the axes of the space. However, if the axes are not orthogonal, calculations will tend to be complicated not to mention that we usually attribute to each vector of the basis to have length one (1.0).</p>
</section>
<section id="eigenvalues-and-eigenvectors">
<h3>Eigenvalues and Eigenvectors<a class="headerlink" href="#eigenvalues-and-eigenvectors" title="Permalink to this headline">#</a></h3>
<p>The following video gives an intuitive explanation of eigenvalues and eigenvectors and its included here due to its visualizations that it offers.  The video must be viewed in conjunction with <a class="reference external" href="http://math.mit.edu/~gs/linearalgebra/linearalgebra5_6-1.pdf">Strang’s introduction</a></p>
<iframe width="560" height="315" src="https://www.youtube.com/embed/PFDu9oVAE-g" frameborder="0" allow="accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>
<p>During the lecture we will go through an example from how your brain processes the sensory input generated by the voice of the lecturer(unless you are already asleep by that time) to combine <em>optimally</em> the sound from both your ears.</p>
<p>A geometric interpretation of the eigenvectors and eigenvalues is given in the following figure:</p>
<p><img alt="eigenvectors" src="../../../_images/eigenvectors.png" /></p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./aiml-common/lectures/linear-algebra"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
    <a class='left-prev' id="prev-link" href="../ml-math/_index.html" title="previous page">
        <i class="fas fa-angle-left"></i>
        <div class="prev-next-info">
            <p class="prev-next-subtitle">previous</p>
            <p class="prev-next-title">Math for ML</p>
        </div>
    </a>
    <a class='right-next' id="next-link" href="../ml-math/calculus/_index.html" title="next page">
    <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">Calculus</p>
    </div>
    <i class="fas fa-angle-right"></i>
    </a>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Pantelis Monogioudis, Ph.D<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../../../_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>