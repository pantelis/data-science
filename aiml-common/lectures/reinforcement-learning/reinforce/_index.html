
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>The REINFORCE Algorithm &#8212; Introduction to Data Science</title>
    
  <link href="../../../../_static/css/theme.css" rel="stylesheet">
  <link href="../../../../_static/css/index.ff1ffe594081f20da1ef19478df9384b.css" rel="stylesheet">

    
  <link rel="stylesheet"
    href="../../../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" type="text/css" href="../../../../_static/pygments.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js">

    <script data-url_root="../../../../" id="documentation_options" src="../../../../_static/documentation_options.js"></script>
    <script src="../../../../_static/jquery.js"></script>
    <script src="../../../../_static/underscore.js"></script>
    <script src="../../../../_static/doctools.js"></script>
    <script src="../../../../_static/togglebutton.js"></script>
    <script src="../../../../_static/clipboard.min.js"></script>
    <script src="../../../../_static/copybutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../../../_static/sphinx-book-theme.d59cb220de22ca1c485ebbdc042f0030.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script>
        const thebe_selector = ".thebe"
        const thebe_selector_input = "pre"
        const thebe_selector_output = ".output"
    </script>
    <script async="async" src="../../../../_static/sphinx-thebe.js"></script>
    <link rel="canonical" href="https://pantelis.dev/aiml-common/lectures/reinforcement-learning/reinforce/_index.html" />
    <link rel="index" title="Index" href="../../../../genindex.html" />
    <link rel="search" title="Search" href="../../../../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../../../_static/logo.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Data Science</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../../../intro.html">
   Introduction to Data Science
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Introduction to Data Science
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/course-introduction/_index.html">
   Course Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ai-intro/data-science-360/_index.html">
   Data Science 360
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Probabilistic Learning
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml-math/probability/_index.html">
   Probability Basics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../entropy/_index.html">
   Entropy
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/bayesian-inference/_index.html">
   Bayesian Inference
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/covid19-antibody-test/_index.html">
   COVID-19 Antibody Test
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pgm/bayesian-coin/_index.html">
   Bayesian Coin Flipping
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  The Learning Problem
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../regression/linear-regression/_index.html">
   Linear Regression
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../optimization/sgd/_index.html">
   Stochastic Gradient Descent
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Classification
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/classification-intro/_index.html">
   Introduction to Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/logistic-regression/_index.html">
   Logistic Regression
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Trees and Intelligence of the Crowds
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../decision-trees/_index.html">
   Decision Trees
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ensemble/_index.html">
   Ensemble Methods
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ensemble/random-forests/_index.html">
   Random Forests
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ensemble/boosting/_index.html">
   Boosting
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ensemble/boosting-workshop/_index.html">
   Boosting workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Non-Parametric Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../unsupervised/k-means/_index.html">
   K-means Clustering
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../density-estimation/knn/_index.html">
   k-Nearest Neighbors (kNN) Classification
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../density-estimation/knn-workshop/_index.html">
   kNN Workshop
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Neural Networks
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../classification/perceptron/_index.html">
   The Neuron (Perceptron)
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/dnn-intro/_index.html">
   Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/backprop-intro/_index.html">
   Introduction to Backpropagation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../dnn/backprop-dnn/_index.html">
   Backpropagation in Deep Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-intro/_index.html">
   Introduction to Convolutional Neural Networks
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-layers/_index.html">
   CNN Architectures
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../cnn/cnn-example-architectures/_index.html">
   CNN Example Architectures
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Dimensionality Reduction
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../recommenders/recommenders-intro/_index.html">
   Recommender System Introduction
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../recommenders/netflix/_index.html">
   The Netflix Prize and Singular Value Decomposition
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../pca/_index.html">
   Principal Component Analysis (PCA)
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Math Background
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../linear-algebra/vectors/_index.html">
   Vectors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../linear-algebra/matrices/_index.html">
   Matrices
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="../../ml-math/calculus/_index.html">
   Calculus
  </a>
 </li>
</ul>
<p aria-level="2" class="caption" role="heading">
 <span class="caption-text">
  Resources
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="../../python/_index.html">
   Learn Python
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../../../_sources/aiml-common/lectures/reinforcement-learning/reinforce/_index.md"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.md</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
                onclick="printPdf(this)" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Connect with source repository"><i class="fab fa-github"></i></button>
    <div class="dropdown-buttons sourcebuttons">
        <a class="repository-button"
            href="https://github.com/executablebooks/jupyter-book"><button type="button" class="btn btn-secondary topbarbtn"
                data-toggle="tooltip" data-placement="left" title="Source repository"><i
                    class="fab fa-github"></i>repository</button></a>
        <a class="issues-button"
            href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Faiml-common/lectures/reinforcement-learning/reinforce/_index.html&body=Your%20issue%20content%20here."><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Open an issue"><i class="fas fa-lightbulb"></i>open issue</button></a>
        <a class="edit-button" href="https://github.com/executablebooks/jupyter-book/edit/master/docs/aiml-common/lectures/reinforcement-learning/reinforce/_index.md"><button
                type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip" data-placement="left"
                title="Edit this page"><i class="fas fa-pencil-alt"></i>suggest edit</button></a>
    </div>
</div>

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show noprint">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-network">
   Policy Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applying-the-reinforce-algorithm">
   Applying the REINFORCE algorithm
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>The REINFORCE Algorithm</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#policy-network">
   Policy Network
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#applying-the-reinforce-algorithm">
   Applying the REINFORCE algorithm
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            
              <div>
                
  <div class="tex2jax_ignore mathjax_ignore section" id="the-reinforce-algorithm">
<h1>The REINFORCE Algorithm<a class="headerlink" href="#the-reinforce-algorithm" title="Permalink to this headline">¶</a></h1>
<p>Given that RL can be posed as an MDP, in this section we continue with a policy-based algorithm that learns the policy <em>directly</em> by optimizing the objective function and can then map the states to actions.  The algorithm we treat here, called REINFORCE, is important although more modern algorithms do perform better.</p>
<p>It took its name from the fact that during <em>training</em> actions that resulted in good outcomes should become more probable—these actions are positively <em>reinforced</em>. Conversely, actions which resulted in bad outcomes should become less probable. If learning is successful, over the course of many iterations, action probabilities produced by the policy, shift to a distribution that results in good performance in an environment. Action probabilities are changed by following the policy gradient, therefore REINFORCE is known as a <em>policy gradient</em> algorithm.</p>
<p>The algorithm needs three components:</p>
<table class="colwidths-auto table">
<thead>
<tr class="row-odd"><th class="head"><p>Component</p></th>
<th class="head"><p>Description</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>Parametrized policy <span class="math notranslate nohighlight">\(\pi_\theta (a|s)\)</span></p></td>
<td><p>The key idea of the algorithm is to learn a good policy, and this means doing function approximation. Neural networks are powerful and flexible function approximators, so we can represent a policy using a deep neural network (DNN) consisting of learnable parameters <span class="math notranslate nohighlight">\(\mathbf \theta\)</span>. This is often referred to as a policy network <span class="math notranslate nohighlight">\(\pi_θ\)</span>. We say that the policy is parametrized by  <span class="math notranslate nohighlight">\(\theta\)</span>. Each specific set of values of the parameters of the policy network represents a particular policy. To see why, consider <span class="math notranslate nohighlight">\(θ_1 ≠ θ_2\)</span>. For any given state <span class="math notranslate nohighlight">\(s\)</span>, different policy networks may output different sets of action probabilities, that is, <span class="math notranslate nohighlight">\(\pi_{θ_1}(a | s) \neq \pi_{θ_2}(a | s)\)</span>. The mappings from states to action probabilities are different so we say that <span class="math notranslate nohighlight">\(π_{θ_1}\)</span> and <span class="math notranslate nohighlight">\(π_{θ_2}\)</span> are different policies. A single DNN is therefore capable of representing many different policies.</p></td>
</tr>
<tr class="row-odd"><td><p>The objective to be maximized <span class="math notranslate nohighlight">\(J(\pi_\theta)\)</span><a class="footnote-reference brackets" href="#id3" id="id1">1</a></p></td>
<td><p>At this point is nothing else other than the expected discounted <em>return</em> over policy, just like in MDP.</p></td>
</tr>
<tr class="row-even"><td><p>Policy Gradient</p></td>
<td><p>A method for updating the policy parameters <span class="math notranslate nohighlight">\(\theta\)</span>. The policy gradient algorithm searches for a local maximum in <span class="math notranslate nohighlight">\(J(\pi_\theta)\)</span>:  <span class="math notranslate nohighlight">\(\max_\theta J(\pi_\theta)\)</span>. This is the common gradient ascent algorithm that we met in a similar form in neural networks. $<span class="math notranslate nohighlight">\(\theta ← \theta + \alpha \nabla_\theta J(\pi_\theta)\)</span><span class="math notranslate nohighlight">\( where \)</span>\alpha$ is the learning rate.</p></td>
</tr>
</tbody>
</table>
<p>Out of the three components, the most complicated one is the policy gradient that can be shown to be given by the differentiable quantity:</p>
<div class="math notranslate nohighlight">
\[ \nabla_\theta J(\pi_\theta)= \mathbb{E}_{\pi_\theta} \left[ \nabla_\theta \log \pi_\theta (a|s) v_\pi (s) \right ]\]</div>
<p>We understand that this expression came out of nowhere but the interested reader can find its detailed derivation in the chapter 2 of <a class="reference external" href="https://www.amazon.com/Deep-Reinforcement-Learning-Python-Hands/dp/0135172381">this</a> reference. We can approximate the value at state <span class="math notranslate nohighlight">\(s\)</span> with the return over many sample trajectories <span class="math notranslate nohighlight">\(\tau\)</span> that are sampled from the policy network.</p>
<div class="math notranslate nohighlight">
\[ \nabla_\theta J(\pi_\theta)= \mathbb{E}_{\tau \sim \pi_\theta} \left[ G_t \nabla_\theta \log \pi_\theta (a|s) \right ]\]</div>
<p>where <span class="math notranslate nohighlight">\(G_t\)</span> is the <em>return</em> - a quantity we have seen earlier albeit now the return is limited by the length of each trajectory just like in MC method,</p>
<div class="math notranslate nohighlight">
\[G_t(\tau) = \sum_{k=0}^{T-1}\gamma^k R_{t+1+k}\]</div>
<p>The <span class="math notranslate nohighlight">\(\gamma\)</span> is usually a hyper-parameter that we need to optimize usually iterating over many values in [0.01,…,0.99] and selecting the one with the best results.</p>
<p>We also have an expectation in the gradient expression that we need to address.  The expectation <span class="math notranslate nohighlight">\(\mathbb E_{\tau \sim \pi_\theta}\)</span> we need to take is approximated with a summation over <em>each</em> trajectory aka a Monte-Carlo approximation. Effectively, we are generating the right hand side as in line 8 in the code below, by sampling a trajectory (line 4) and estimating its return (line 7) in a completely model-free fashion i.e. without assuming any knowledge of the transition and reward functions. This is implemented next:</p>
<p>1: Initialize learning rate <span class="math notranslate nohighlight">\(\alpha\)</span></p>
<p>2: Initialize weights θ of a policy network <span class="math notranslate nohighlight">\(π_θ\)</span></p>
<p>3: for episode = 0, … , MAX_EPISODE do</p>
<p>4:   Sample a trajectory using the policy network <span class="math notranslate nohighlight">\(\pi_\theta\)</span>, <span class="math notranslate nohighlight">\(τ = s_0, a_0, r_0, . . . , s_T, a_T, r_T\)</span></p>
<p>5:   Set <span class="math notranslate nohighlight">\(∇_θ J(π_θ) = 0\)</span></p>
<p>6:    for t = 0, … , T-1 do</p>
<p>7:               Calculate <span class="math notranslate nohighlight">\(G_t(τ)\)</span></p>
<p>8:               <span class="math notranslate nohighlight">\(\nabla_\theta J(\pi_\theta) = \nabla_\theta J(\pi_\theta) + G_t (τ) \nabla_\theta \log \pi_\theta (a_t|s_t) \)</span></p>
<p>9:        end for</p>
<p>10:        <span class="math notranslate nohighlight">\(θ = θ + α ∇_θ J(π_θ)\)</span></p>
<p>11: end for</p>
<p>It is important that a trajectory is discarded after each parameter update—it cannot be reused. This is because REINFORCE is an <em>on-policy</em> algorithm just like the MC it “learns on the job”. This is evidently seen in line 10 where the parameter update equation uses the policy gradient that itself (line 8) directly depends on action probabilities <span class="math notranslate nohighlight">\(π_θ(a_t | s_t)\)</span> generated by the <em>current</em> policy <span class="math notranslate nohighlight">\(π_θ\)</span> only and not some past policy <span class="math notranslate nohighlight">\(π_{θ′}\)</span>. Correspondingly, the return <span class="math notranslate nohighlight">\(G_t(τ)\)</span> where <span class="math notranslate nohighlight">\(τ ~ π_θ\)</span> must also be generated from <span class="math notranslate nohighlight">\(π_θ\)</span>, otherwise the action probabilities will be adjusted based on returns that the policy wouldn’t have generated.</p>
<div class="section" id="policy-network">
<h2>Policy Network<a class="headerlink" href="#policy-network" title="Permalink to this headline">¶</a></h2>
<p>One of the key ingredients that REINFORCE introduces is the policy network that is approximated with a DNN eg. a fully connected neural network with a number of hidden layers that is hyper-parameter (e.g. 2 RELU layers).</p>
<p>1: Given a policy network <code class="docutils literal notranslate"><span class="pre">net</span></code>, a <code class="docutils literal notranslate"><span class="pre">Categorical</span></code> (multinomial) distribution class, and a <code class="docutils literal notranslate"><span class="pre">state</span></code></p>
<p>2: Compute the output <code class="docutils literal notranslate"><span class="pre">pdparams</span> <span class="pre">=</span> <span class="pre">net(state)</span></code></p>
<p>3: Construct an instance of an action probability distribution  <code class="docutils literal notranslate"><span class="pre">pd</span> <span class="pre">=</span> <span class="pre">Categorical(logits=pdparams)</span></code></p>
<p>4: Use pd to sample an action, <code class="docutils literal notranslate"><span class="pre">action</span> <span class="pre">=</span> <span class="pre">pd.sample()</span></code></p>
<p>5: Use pd and action to compute the action log probability, <code class="docutils literal notranslate"><span class="pre">log_prob</span> <span class="pre">=</span> <span class="pre">pd.log_prob(action)</span></code></p>
<p>Other discrete distributions can be used and many actual libraries parametrize continuous distributions such as Gaussians.</p>
</div>
<div class="section" id="applying-the-reinforce-algorithm">
<h2>Applying the REINFORCE algorithm<a class="headerlink" href="#applying-the-reinforce-algorithm" title="Permalink to this headline">¶</a></h2>
<p>It is now instructive to see an stand-alone example in python for the so called <code class="docutils literal notranslate"><span class="pre">CartPole-v0</span></code> <a class="footnote-reference brackets" href="#id4" id="id2">2</a></p>
<p><img alt="cartpole" src="aiml-common/lectures/reinforcement-learning/reinforce/images/cartpole.png#center" /></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span> <span class="mi">1</span>  <span class="kn">from</span> <span class="nn">torch.distributions</span> <span class="kn">import</span> <span class="n">Categorical</span>
 <span class="mi">2</span>  <span class="kn">import</span> <span class="nn">gym</span>
 <span class="mi">3</span>  <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
 <span class="mi">4</span>  <span class="kn">import</span> <span class="nn">torch</span>
 <span class="mi">5</span>  <span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
 <span class="mi">6</span>  <span class="kn">import</span> <span class="nn">torch.optim</span> <span class="k">as</span> <span class="nn">optim</span>
 <span class="mi">7</span>
 <span class="mi">8</span>  <span class="n">gamma</span> <span class="o">=</span> <span class="mf">0.99</span>
 <span class="mi">9</span>
<span class="mi">10</span>  <span class="k">class</span> <span class="nc">Pi</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
<span class="mi">11</span>      <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">):</span>
<span class="mi">12</span>          <span class="nb">super</span><span class="p">(</span><span class="n">Pi</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
<span class="mi">13</span>          <span class="n">layers</span> <span class="o">=</span> <span class="p">[</span>
<span class="mi">14</span>              <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="mi">64</span><span class="p">),</span>
<span class="mi">15</span>              <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(),</span>
<span class="mi">16</span>              <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">64</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">),</span>
<span class="mi">17</span>          <span class="p">]</span>
<span class="mi">18</span>          <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span><span class="o">*</span><span class="n">layers</span><span class="p">)</span>
<span class="mi">19</span>          <span class="bp">self</span><span class="o">.</span><span class="n">onpolicy_reset</span><span class="p">()</span>
<span class="mi">20</span>          <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">()</span> <span class="c1"># set training mode</span>
<span class="mi">21</span>
<span class="mi">22</span>      <span class="k">def</span> <span class="nf">onpolicy_reset</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
<span class="mi">23</span>          <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span> <span class="o">=</span> <span class="p">[]</span>
<span class="mi">24</span>          <span class="bp">self</span><span class="o">.</span><span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="mi">25</span>
<span class="mi">26</span>      <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
<span class="mi">27</span>          <span class="n">pdparam</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="mi">28</span>          <span class="k">return</span> <span class="n">pdparam</span>
<span class="mi">29</span>
<span class="mi">30</span>      <span class="k">def</span> <span class="nf">act</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">state</span><span class="p">):</span>
<span class="mi">31</span>          <span class="n">x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">state</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">))</span> <span class="c1"># to tensor</span>
<span class="mi">32</span>          <span class="n">pdparam</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1"># forward pass</span>
<span class="mi">33</span>          <span class="n">pd</span> <span class="o">=</span> <span class="n">Categorical</span><span class="p">(</span><span class="n">logits</span><span class="o">=</span><span class="n">pdparam</span><span class="p">)</span> <span class="c1"># probability distribution</span>
<span class="mi">34</span>          <span class="n">action</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">sample</span><span class="p">()</span> <span class="c1"># pi(a|s) in action via pd</span>
<span class="mi">35</span>          <span class="n">log_prob</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">log_prob</span><span class="p">(</span><span class="n">action</span><span class="p">)</span> <span class="c1"># log_prob of pi(a|s)</span>
<span class="mi">36</span>          <span class="bp">self</span><span class="o">.</span><span class="n">log_probs</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">log_prob</span><span class="p">)</span> <span class="c1"># store for training</span>
<span class="mi">37</span>          <span class="k">return</span> <span class="n">action</span><span class="o">.</span><span class="n">item</span><span class="p">()</span>
<span class="mi">38</span>
<span class="mi">39</span>  <span class="k">def</span> <span class="nf">train</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">):</span>
<span class="mi">40</span>      <span class="c1"># Inner gradient-ascent loop of REINFORCE algorithm</span>
<span class="mi">41</span>      <span class="n">T</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>
<span class="mi">42</span>      <span class="n">rets</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">(</span><span class="n">T</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span> <span class="c1"># the returns</span>
<span class="mi">43</span>      <span class="n">future_ret</span> <span class="o">=</span> <span class="mf">0.0</span>
<span class="mi">44</span>      <span class="c1"># compute the returns efficiently</span>
<span class="mi">45</span>      <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="p">)):</span>
<span class="mi">46</span>          <span class="n">future_ret</span> <span class="o">=</span> <span class="n">pi</span><span class="o">.</span><span class="n">rewards</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">+</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">future_ret</span>
<span class="mi">47</span>          <span class="n">rets</span><span class="p">[</span><span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">future_ret</span>
<span class="mi">48</span>      <span class="n">rets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">rets</span><span class="p">)</span>
<span class="mi">49</span>      <span class="n">log_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">log_probs</span><span class="p">)</span>
<span class="mi">50</span>      <span class="n">loss</span> <span class="o">=</span> <span class="o">-</span> <span class="n">log_probs</span> <span class="o">*</span> <span class="n">rets</span> <span class="c1"># gradient term; Negative for maximizing</span>
<span class="mi">51</span>      <span class="n">loss</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">loss</span><span class="p">)</span>
<span class="mi">52</span>      <span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span>
<span class="mi">53</span>      <span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span> <span class="c1"># backpropagate, compute gradients</span>
<span class="mi">54</span>      <span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span> <span class="c1"># gradient-ascent, update the weights</span>
<span class="mi">55</span>      <span class="k">return</span> <span class="n">loss</span>
<span class="mi">56</span>
<span class="mi">57</span>  <span class="k">def</span> <span class="nf">main</span><span class="p">():</span>
<span class="mi">58</span>      <span class="n">env</span> <span class="o">=</span> <span class="n">gym</span><span class="o">.</span><span class="n">make</span><span class="p">(</span><span class="s1">&#39;CartPole-v0&#39;</span><span class="p">)</span>
<span class="mi">59</span>      <span class="n">in_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">observation_space</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="c1"># 4</span>
<span class="mi">60</span>      <span class="n">out_dim</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">action_space</span><span class="o">.</span><span class="n">n</span> <span class="c1"># 2</span>
<span class="mi">61</span>      <span class="n">pi</span> <span class="o">=</span> <span class="n">Pi</span><span class="p">(</span><span class="n">in_dim</span><span class="p">,</span> <span class="n">out_dim</span><span class="p">)</span> <span class="c1"># policy pi_theta for REINFORCE</span>
<span class="mi">62</span>      <span class="n">optimizer</span> <span class="o">=</span> <span class="n">optim</span><span class="o">.</span><span class="n">Adam</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span><span class="o">=</span><span class="mf">0.01</span><span class="p">)</span>
<span class="mi">63</span>      <span class="k">for</span> <span class="n">epi</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">300</span><span class="p">):</span>
<span class="mi">64</span>          <span class="n">state</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">reset</span><span class="p">()</span>
<span class="mi">65</span>          <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">200</span><span class="p">):</span> <span class="c1"># cartpole max timestep is 200</span>
<span class="mi">66</span>              <span class="n">action</span> <span class="o">=</span> <span class="n">pi</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">state</span><span class="p">)</span>
<span class="mi">67</span>              <span class="n">state</span><span class="p">,</span> <span class="n">reward</span><span class="p">,</span> <span class="n">done</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">env</span><span class="o">.</span><span class="n">step</span><span class="p">(</span><span class="n">action</span><span class="p">)</span>
<span class="mi">68</span>              <span class="n">pi</span><span class="o">.</span><span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
<span class="mi">69</span>              <span class="n">env</span><span class="o">.</span><span class="n">render</span><span class="p">()</span>
<span class="mi">70</span>              <span class="k">if</span> <span class="n">done</span><span class="p">:</span>
<span class="mi">71</span>                  <span class="k">break</span>
<span class="mi">72</span>          <span class="n">loss</span> <span class="o">=</span> <span class="n">train</span><span class="p">(</span><span class="n">pi</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">)</span> <span class="c1"># train per episode</span>
<span class="mi">73</span>          <span class="n">total_reward</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="n">pi</span><span class="o">.</span><span class="n">rewards</span><span class="p">)</span>
<span class="mi">74</span>          <span class="n">solved</span> <span class="o">=</span> <span class="n">total_reward</span> <span class="o">&gt;</span> <span class="mf">195.0</span>
<span class="mi">75</span>          <span class="n">pi</span><span class="o">.</span><span class="n">onpolicy_reset</span><span class="p">()</span> <span class="c1"># onpolicy: clear memory after training</span>
<span class="mi">76</span>          <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s1">&#39;Episode </span><span class="si">{</span><span class="n">epi</span><span class="si">}</span><span class="s1">, loss: </span><span class="si">{</span><span class="n">loss</span><span class="si">}</span><span class="s1">, </span><span class="se">\</span>
<span class="s1">77          total_reward: </span><span class="si">{</span><span class="n">total_reward</span><span class="si">}</span><span class="s1">, solved: </span><span class="si">{</span><span class="n">solved</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="mi">78</span>
<span class="mi">79</span>  <span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s1">&#39;__main__&#39;</span><span class="p">:</span>
<span class="mi">80</span>      <span class="n">main</span><span class="p">()</span>
</pre></div>
</div>
<p>The REINFORCE algorithm presented here can generally be applied to continuous and discreet problems but it has been shown to possess high variance and sample-inefficiency. Several improvements have been proposed and the interested reader can refer to section 2.5.1 of the suggested book.</p>
<hr class="footnotes docutils" />
<dl class="footnote brackets">
<dt class="label" id="id3"><span class="brackets"><a class="fn-backref" href="#id1">1</a></span></dt>
<dd><p>Notation wise, since we need to have a bit more flexibility in RL problems, we will use the symbol <span class="math notranslate nohighlight">\(J(\pi_\theta)\)</span> as the objective function.</p>
</dd>
<dt class="label" id="id4"><span class="brackets"><a class="fn-backref" href="#id2">2</a></span></dt>
<dd><p>Please note that SLM-Lab, the library that accompanies the suggested in the syllabus book, is a mature library and probably a good example of how to develop ML/RL libraries in python. You will learn a lot by reviewing the implementations under the <code class="docutils literal notranslate"><span class="pre">agents/algorithms</span></code> directory to get a feel of how RL problems are abstracted .</p>
</dd>
</dl>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./aiml-common/lectures/reinforcement-learning/reinforce"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            
                <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            
        </div>
    </div>
    <footer class="footer">
  <p>
    
      By Pantelis Monogioudis, Ph.D<br/>
    
        &copy; Copyright 2021.<br/>
  </p>
</footer>
</main>


      </div>
    </div>
  
  <script src="../../../../_static/js/index.be7d3bbb2ef33a8344ce.js"></script>

  </body>
</html>